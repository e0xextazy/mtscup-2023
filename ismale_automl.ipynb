{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./LightAutoML-0.3.7.4-py3-none-any.whl\n",
      "Collecting scikit-learn<=0.24.2,>=0.22\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from LightAutoML==0.3.7.4) (3.1.2)\n",
      "Collecting lightgbm<=3.2.1,>=2.3\n",
      "  Downloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting poetry-core<2.0.0,>=1.0.0\n",
      "  Downloading poetry_core-1.5.2-py3-none-any.whl (465 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.2/465.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting json2html\n",
      "  Downloading json2html-1.3.0.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from LightAutoML==0.3.7.4) (1.2.0)\n",
      "Collecting cmaes\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting numpy<1.24.0\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/site-packages (from LightAutoML==0.3.7.4) (6.0)\n",
      "Collecting pandas<=1.4.3\n",
      "  Downloading pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch<1.9\n",
      "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting catboost>=0.26.1\n",
      "  Downloading catboost-1.1.1-cp38-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting holidays\n",
      "  Downloading holidays-0.21.13-py3-none-any.whl (378 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.2/378.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from LightAutoML==0.3.7.4) (4.65.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/site-packages (from LightAutoML==0.3.7.4) (3.0)\n",
      "Collecting autowoe>=1.2\n",
      "  Downloading AutoWoE-1.3.2-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from autowoe>=1.2->LightAutoML==0.3.7.4) (1.10.1)\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sphinx-rtd-theme\n",
      "  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.8/site-packages (from autowoe>=1.2->LightAutoML==0.3.7.4) (2022.7.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting StrEnum<0.5.0,>=0.4.7\n",
      "  Downloading StrEnum-0.4.9-py3-none-any.whl (7.6 kB)\n",
      "Collecting sphinx\n",
      "  Downloading sphinx-6.1.3-py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from catboost>=0.26.1->LightAutoML==0.3.7.4) (1.16.0)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting plotly\n",
      "  Downloading plotly-5.13.1-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from lightgbm<=3.2.1,>=2.3->LightAutoML==0.3.7.4) (0.38.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/site-packages (from pandas<=1.4.3->LightAutoML==0.3.7.4) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn<=0.24.2,>=0.22->LightAutoML==0.3.7.4) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch<1.9->LightAutoML==0.3.7.4) (4.5.0)\n",
      "Collecting korean-lunar-calendar\n",
      "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
      "Collecting convertdate>=2.3.0\n",
      "  Downloading convertdate-2.4.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backports.zoneinfo\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hijri-converter\n",
      "  Downloading hijri_converter-2.2.4-py3-none-any.whl (14 kB)\n",
      "Collecting PyMeeus\n",
      "  Downloading PyMeeus-0.5.12.tar.gz (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->LightAutoML==0.3.7.4) (2.1.2)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from optuna->LightAutoML==0.3.7.4) (23.0)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from torchvision->LightAutoML==0.3.7.4) (2.28.2)\n",
      "  Downloading torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl (24.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.10.1-cp38-cp38-manylinux1_x86_64.whl (22.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.10.0-cp38-cp38-manylinux1_x86_64.whl (22.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Downloading torchvision-0.9.1-cp38-cp38-manylinux1_x86_64.whl (17.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/site-packages (from alembic>=1.5.0->optuna->LightAutoML==0.3.7.4) (6.0.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/site-packages (from alembic>=1.5.0->optuna->LightAutoML==0.3.7.4) (5.12.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (618 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.5/618.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/site-packages (from pytest->autowoe>=1.2->LightAutoML==0.3.7.4) (2.0.1)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/site-packages (from pytest->autowoe>=1.2->LightAutoML==0.3.7.4) (22.2.0)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting snowballstemmer>=2.0\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.8/site-packages (from sphinx->autowoe>=1.2->LightAutoML==0.3.7.4) (2.14.0)\n",
      "Collecting docutils<0.20,>=0.18\n",
      "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.8/site-packages (from sphinx->autowoe>=1.2->LightAutoML==0.3.7.4) (2.12.1)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting imagesize>=1.3\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docutils<0.20,>=0.18\n",
      "  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.0/570.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna->LightAutoML==0.3.7.4) (3.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->torchvision->LightAutoML==0.3.7.4) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->torchvision->LightAutoML==0.3.7.4) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->torchvision->LightAutoML==0.3.7.4) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->torchvision->LightAutoML==0.3.7.4) (3.4)\n",
      "Building wheels for collected packages: json2html, PyMeeus\n",
      "  Building wheel for json2html (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7609 sha256=d0c5d2fa2dbab9c0bb0a3ceac8fa8623056bb8faeee47ab725c478c6bc2c3dc1\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/1a/a9/0d39b9e11fc97dd947ca4df7f0f0c1f4f2e25727d3c63b7739\n",
      "  Building wheel for PyMeeus (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyMeeus: filename=PyMeeus-0.5.12-py3-none-any.whl size=732015 sha256=19dfbedacdd8b507d35a970809056264007e543c48790984efdc1007c62e498c\n",
      "  Stored in directory: /root/.cache/pip/wheels/c2/3a/3d/11734e652782d3f823a08aae1c452e887eb16349750cca3f8a\n",
      "Successfully built json2html PyMeeus\n",
      "Installing collected packages: StrEnum, snowballstemmer, PyMeeus, korean-lunar-calendar, json2html, tenacity, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, pyparsing, poetry-core, pluggy, pillow, numpy, Mako, kiwisolver, iniconfig, imagesize, hijri-converter, greenlet, graphviz, fonttools, exceptiongroup, docutils, cycler, convertdate, colorlog, backports.zoneinfo, alabaster, torch, sqlalchemy, sphinx, pytest, plotly, pandas, holidays, contourpy, cmaes, torchvision, sphinxcontrib-jquery, scikit-learn, matplotlib, alembic, sphinx-rtd-theme, seaborn, optuna, lightgbm, catboost, autowoe, LightAutoML\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script mako-render is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script censusgeocode is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts sphinx-apidoc, sphinx-autogen, sphinx-build and sphinx-quickstart are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script alembic is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script optuna is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.8.1 which is incompatible.\n",
      "pytorch-lifestream 0.5.1 requires pytorch-lightning==1.6.*, but you have pytorch-lightning 1.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed LightAutoML-0.3.7.4 Mako-1.2.4 PyMeeus-0.5.12 StrEnum-0.4.9 alabaster-0.7.13 alembic-1.10.2 autowoe-1.3.2 backports.zoneinfo-0.2.1 catboost-1.1.1 cmaes-0.9.1 colorlog-6.7.0 contourpy-1.0.7 convertdate-2.4.0 cycler-0.11.0 docutils-0.18.1 exceptiongroup-1.1.1 fonttools-4.39.0 graphviz-0.20.1 greenlet-2.0.2 hijri-converter-2.2.4 holidays-0.21.13 imagesize-1.4.1 iniconfig-2.0.0 json2html-1.3.0 kiwisolver-1.4.4 korean-lunar-calendar-0.3.1 lightgbm-3.2.1 matplotlib-3.7.1 numpy-1.23.5 optuna-3.1.0 pandas-1.4.3 pillow-9.4.0 plotly-5.13.1 pluggy-1.0.0 poetry-core-1.5.2 pyparsing-3.0.9 pytest-7.2.2 scikit-learn-0.24.2 seaborn-0.12.2 snowballstemmer-2.2.0 sphinx-6.1.3 sphinx-rtd-theme-1.2.0 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 sqlalchemy-2.0.6 tenacity-8.2.2 torch-1.8.1 torchvision-0.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U --user LightAutoML-0.3.7.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import lightautoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 27 09:10:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   47C    P8    38W / 450W |   1017MiB / 24564MiB |     15%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270000\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train_ml_num_v2.csv\")\n",
    "test = pd.read_csv(\"test_ml_num_v2.csv\")\n",
    "test.sort_values(\"user_id\", inplace=True)\n",
    "cold = False\n",
    "\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263065\n"
     ]
    }
   ],
   "source": [
    "warm_features = ['social_media last', 'social_media first', 'sum_text last', 'sum_text first', 'urls_topics last', 'urls_topics first', 'url_host_mc', 'url_host_2_mc', 'url_host_3_mc', 'urls_topics_mc', 'url_host_day_mc', 'url_host_pod_mc', 'request_cnt max', 'request_cnt mean', 'request_cnt std', 'social_media sum', '339 last', '339 first', '68 last', '68 first', '215 last', '215 first', '503 last', '503 first', '251 last', '251 first', '316 last', '316 first', '314 last', '314 first', '418 last', '418 first', '284 last', '284 first', '289 last', '289 first', '415 last', '415 first', '765 last', '765 first', '165 last', '165 first', '556 last', '556 first', '391 last', '391 first', '535 last', '535 first', '385 last', '385 first', '147 last', '147 first', '25 last', '25 first', '526 last', '526 first', '40 last', '40 first', '199 last', '199 first', '583 last', '583 first', '273 last', '273 first', '169 last', '169 first', '15 last', '15 first', '592 last', '592 first', '268 last', '268 first', '437 last', '437 first', '315 last', '315 first', '893 last', '893 first', '493 last', '493 first', '465 last', '465 first', '1450 last', '1450 first', '794 last', '794 first', '1819 last', '1819 first','als_emb_url_host_day_0', 'als_emb_url_host_day_1', 'als_emb_url_host_day_2', 'als_emb_url_host_day_3', 'als_emb_url_host_day_4', 'als_emb_url_host_day_5', 'als_emb_url_host_day_6', 'als_emb_url_host_day_7', 'als_emb_url_host_day_8', 'als_emb_url_host_day_9', 'als_emb_url_host_day_10', 'als_emb_url_host_day_11', 'als_emb_url_host_day_12', 'als_emb_url_host_day_13', 'als_emb_url_host_day_14', 'als_emb_url_host_day_15', 'als_emb_url_host_day_16', 'als_emb_url_host_day_17', 'als_emb_url_host_day_18', 'als_emb_url_host_day_19', 'als_emb_url_host_day_20', 'als_emb_url_host_day_21', 'als_emb_url_host_day_22', 'als_emb_url_host_day_23', 'als_emb_url_host_day_24', 'als_emb_url_host_day_25', 'als_emb_url_host_day_26', 'als_emb_url_host_day_27', 'als_emb_url_host_day_28', 'als_emb_url_host_day_29', 'als_emb_url_host_day_30', 'als_emb_url_host_day_31', 'url_host_day_clusters_age', 'url_host_day_clusters_ismale', 'als_emb_url_host_pod_0', 'als_emb_url_host_pod_1', 'als_emb_url_host_pod_2', 'als_emb_url_host_pod_3', 'als_emb_url_host_pod_4', 'als_emb_url_host_pod_5', 'als_emb_url_host_pod_6', 'als_emb_url_host_pod_7', 'als_emb_url_host_pod_8', 'als_emb_url_host_pod_9', 'als_emb_url_host_pod_10', 'als_emb_url_host_pod_11', 'als_emb_url_host_pod_12', 'als_emb_url_host_pod_13', 'als_emb_url_host_pod_14', 'als_emb_url_host_pod_15', 'url_host_pod_clusters_age', 'url_host_pod_clusters_ismale', 'mean_req_cnt_mor', 'mean_req_cnt_day', 'mean_req_cnt_eve', 'mean_req_cnt_nig', 'mean_req_cnt_holiday', 'mean_req_cnt_noholiday', 'mean_req_cnt_weekend', 'mean_req_cnt_noweekend', 'mean_req_cnt_date', 'emb_coles_0', 'emb_coles_1', 'emb_coles_2', 'emb_coles_3', 'emb_coles_4', 'emb_coles_5', 'emb_coles_6', 'emb_coles_7', 'emb_coles_8', 'emb_coles_9', 'emb_coles_10', 'emb_coles_11', 'emb_coles_12', 'emb_coles_13', 'emb_coles_14', 'emb_coles_15', 'emb_coles_16', 'emb_coles_17', 'emb_coles_18', 'emb_coles_19', 'emb_coles_20', 'emb_coles_21', 'emb_coles_22', 'emb_coles_23', 'emb_coles_24', 'emb_coles_25', 'emb_coles_26', 'emb_coles_27', 'emb_coles_28', 'emb_coles_29', 'emb_coles_30', 'emb_coles_31', 'emb_coles_32', 'emb_coles_33', 'emb_coles_34', 'emb_coles_35', 'emb_coles_36', 'emb_coles_37', 'emb_coles_38', 'emb_coles_39', 'emb_coles_40', 'emb_coles_41', 'emb_coles_42', 'emb_coles_43', 'emb_coles_44', 'emb_coles_45', 'emb_coles_46', 'emb_coles_47', 'emb_coles_48', 'emb_coles_49', 'emb_coles_50', 'emb_coles_51', 'emb_coles_52', 'emb_coles_53', 'emb_coles_54', 'emb_coles_55', 'emb_coles_56', 'emb_coles_57', 'emb_coles_58', 'emb_coles_59', 'emb_coles_60', 'emb_coles_61', 'emb_coles_62', 'emb_coles_63', '0_ut', '1_ut', '2_ut', '3_ut', '4_ut', '5_ut', '6_ut', '7_ut', '8_ut', '9_ut', '10_ut', '11_ut', '12_ut', '13_ut', '14_ut', '15_ut', '16_ut', '17_ut', '18_ut', '19_ut', '20_ut', '21_ut', '22_ut', '23_ut', '24_ut', '25_ut', '26_ut', '27_ut', '28_ut', '29_ut', '30_ut', '31_ut', '32_ut', '33_ut', '34_ut', '35_ut', '36_ut', '37_ut', '38_ut', '39_ut', '40_ut', '41_ut', '42_ut', '43_ut', '44_ut', '45_ut', '46_ut', '47_ut', '48_ut', '49_ut', '50_ut', '51_ut', '52_ut', '53_ut', '54_ut', '55_ut', '56_ut', '57_ut', '58_ut', '59_ut', '60_ut', '61_ut', '62_ut', '63_ut', '64_ut', '65_ut', '66_ut', '67_ut', '68_ut', '69_ut', '70_ut', '71_ut', '72_ut', '73_ut', '74_ut', '75_ut', '76_ut', '77_ut', '78_ut', '79_ut', '80_ut', '81_ut', '82_ut', '83_ut', '84_ut', '85_ut', '86_ut', '87_ut', '88_ut', '89_ut', '90_ut', '91_ut', '92_ut', '93_ut', '94_ut', '95_ut', '96_ut', '97_ut', '98_ut', '99_ut', '100_ut', '101_ut', '102_ut', '103_ut', '104_ut', '105_ut', '106_ut', '107_ut', '108_ut', '109_ut', '110_ut', '111_ut', '112_ut', '113_ut', '114_ut', '115_ut', '116_ut', '117_ut', '118_ut', '119_ut', '120_ut', '121_ut', '122_ut', '123_ut', '124_ut', '125_ut', '126_ut', '127_ut', '128_ut', '129_ut', '130_ut', '131_ut', '132_ut', '133_ut', '134_ut', '135_ut', '136_ut', '137_ut', '138_ut', '139_ut', '140_ut', '141_ut', '142_ut', '143_ut', '144_ut', '145_ut', '146_ut', '147_ut', '148_ut', '149_ut', '150_ut', '151_ut', '152_ut', '153_ut', '154_ut', '155_ut', '156_ut', '157_ut', '158_ut', '159_ut', '160_ut', '161_ut', '162_ut', '163_ut', '164_ut', '165_ut', '166_ut', '167_ut', '168_ut', '169_ut', '170_ut', '171_ut', '172_ut', '173_ut', '174_ut', '175_ut', '176_ut', '177_ut', '178_ut', '179_ut', '180_ut', '181_ut', '182_ut', '183_ut', '184_ut', '185_ut', '186_ut', '187_ut', '188_ut', '189_ut', '190_ut', '191_ut', '192_ut', '193_ut', '194_ut', '195_ut', '196_ut', '197_ut', '198_ut', '199_ut', '200_ut', '201_ut', '202_ut', '203_ut', '204_ut', '205_ut', '206_ut', '207_ut', '208_ut', '209_ut', '210_ut', '211_ut', '212_ut', '213_ut', '214_ut', '215_ut', '216_ut', '217_ut', '218_ut', '219_ut', '220_ut', '221_ut', '222_ut', '223_ut', '224_ut', '225_ut', '226_ut', '227_ut', '228_ut', '229_ut', '230_ut', '231_ut', '232_ut', '233_ut', '234_ut', '235_ut', '236_ut', '237_ut', '238_ut', '239_ut', '240_ut', '241_ut', '242_ut', '243_ut', '244_ut', '245_ut', '246_ut', '247_ut', '248_ut', '249_ut', '250_ut', '251_ut', '252_ut', '253_ut', '254_ut', '255_ut', '256_ut', '257_ut', '258_ut', '259_ut', '260_ut', '261_ut', '262_ut', '263_ut', '264_ut', '265_ut', '266_ut', '267_ut', '268_ut', '269_ut', '270_ut', '271_ut', '272_ut', '273_ut', '274_ut', '275_ut', '276_ut', '277_ut', '278_ut', '279_ut', '280_ut', '281_ut', '282_ut', '283_ut', '284_ut', '285_ut', '286_ut', '287_ut', '288_ut', '289_ut', '290_ut', '291_ut', '292_ut', '293_ut', '294_ut', '295_ut', '296_ut', '297_ut', '298_ut', '299_ut', '0_pd', '1_pd', '2_pd', '3_pd', '4_pd', '5_pd', '6_pd', '7_pd', '8_pd', '9_pd', '10_pd', '11_pd', '12_pd', '13_pd', '14_pd', '15_pd', '16_pd', '17_pd', '18_pd', '19_pd', '20_pd', '21_pd', '22_pd', '23_pd', '24_pd', '25_pd', '26_pd', '27_pd', '0_dof', '1_dof', '2_dof', '3_dof', '4_dof', '5_dof', '6_dof', '0_pd.1', '1_pd.1', '2_pd.1', '3_pd.1', 'fe_0', 'fe_1', 'fe_2', 'fe_3', 'fe_4', 'fe_5', 'fe_6', 'fe_7', 'fe_8', 'fe_9', 'fe_10', 'fe_11', 'fe_12', 'fe_13', 'fe_14', 'fe_15', 'fe_16', 'fe_17', 'fe_18', 'fe_19', 'fe_20', 'fe_21', 'fe_22', 'fe_23', 'fe_24', 'fe_25', 'fe_26', 'fe_27', 'fe_28', 'fe_29', 'fe_30', 'fe_31', 'fe_32', 'fe_33', 'fe_34', 'fe_35', 'fe_36', 'fe_37', 'fe_38', 'fe_39', 'fe_40', 'fe_41', 'fe_42', 'fe_43', 'fe_44', 'fe_45', 'fe_46', 'fe_47', 'fe_48', 'fe_49', 'fe_50', 'fe_51', 'fe_52', 'fe_53', 'fe_54', 'fe_55', 'fe_56', 'fe_57', 'fe_58', 'fe_59', 'fe_60', 'fe_61', 'fe_62', 'fe_63', 'fe_64', 'fe_65', 'fe_66', 'fe_67', 'fe_68', 'fe_69', 'fe_70', 'fe_71', 'fe_72', 'fe_73', 'fe_74', 'fe_75', 'fe_76', 'fe_77', 'fe_78', 'fe_79', 'fe_80', 'fe_81', 'fe_82', 'fe_83', 'fe_84', 'fe_85', 'fe_86', 'fe_87', 'fe_88', 'fe_89', 'fe_90', 'fe_91', 'fe_92', 'fe_93', 'fe_94', 'fe_95', 'fe_96', 'fe_97', 'fe_98', 'fe_99', 'fe_100', 'fe_101', 'fe_102', 'fe_103', 'fe_104', 'fe_105', 'fe_106', 'fe_107', 'fe_108', 'fe_109', 'fe_110', 'fe_111', 'fe_112', 'fe_113', 'fe_114', 'fe_115', 'fe_116', 'fe_117', 'fe_118', 'fe_119', 'fe_120', 'fe_121', 'fe_122', 'fe_123', 'fe_124', 'fe_125', 'fe_126', 'fe_127', 'fe_128', 'fe_129', 'fe_130', 'fe_131', 'fe_132', 'fe_133', 'fe_134', 'fe_135', 'fe_136', 'fe_137', 'fe_138', 'fe_139', 'fe_140', 'fe_141', 'fe_142', 'fe_143', 'fe_144', 'fe_145', 'fe_146', 'fe_147', 'fe_148', 'fe_149', 'fe_150', 'fe_151', 'fe_152', 'fe_153', 'fe_154', 'fe_155', 'fe_156', 'fe_157', 'fe_158', 'fe_159', 'fe_160', 'fe_161', 'fe_162', 'fe_163', 'fe_164', 'fe_165', 'fe_166', 'fe_167', 'fe_168', 'fe_169', 'fe_170', 'fe_171', 'fe_172', 'fe_173', 'fe_174', 'fe_175', 'fe_176', 'fe_177', 'fe_178', 'fe_179', 'fe_180', 'fe_181', 'fe_182', 'fe_183', 'fe_184', 'fe_185', 'fe_186', 'fe_187', 'fe_188', 'fe_189', 'fe_190', 'fe_191', 'fe_192', 'fe_193', 'fe_194', 'fe_195', 'fe_196', 'fe_197', 'fe_198', 'fe_199', 'fe_200', 'fe_201', 'fe_202', 'fe_203', 'fe_204', 'fe_205', 'fe_206', 'fe_207', 'fe_208', 'fe_209', 'fe_210', 'fe_211', 'fe_212', 'fe_213', 'fe_214', 'fe_215', 'fe_216', 'fe_217', 'fe_218', 'fe_219', 'fe_220', 'fe_221', 'fe_222', 'fe_223', 'fe_224', 'fe_225', 'fe_226', 'fe_227', 'fe_228', 'fe_229', 'fe_230', 'fe_231', 'fe_232', 'fe_233', 'fe_234', 'fe_235', 'fe_236', 'fe_237', 'fe_238', 'fe_239', 'fe_240', 'fe_241', 'fe_242', 'fe_243', 'fe_244', 'fe_245', 'fe_246', 'fe_247', 'fe_248', 'fe_249', 'fe_250', 'fe_251', 'fe_252', 'fe_253', 'fe_254', 'fe_255', 'fe_256', 'fe_257', 'fe_258', 'fe_259', 'fe_260', 'fe_261', 'fe_262', 'fe_263', 'fe_264', 'fe_265', 'fe_266', 'fe_267', 'fe_268', 'fe_269', 'fe_270', 'fe_271', 'fe_272', 'fe_273', 'fe_274', 'fe_275', 'fe_276', 'fe_277', 'fe_278', 'fe_279', 'fe_280', 'fe_281', 'fe_282', 'fe_283', 'fe_284', 'fe_285', 'fe_286', 'fe_287', 'fe_288', 'fe_289', 'fe_290', 'fe_291', 'fe_292', 'fe_293', 'fe_294', 'fe_295', 'fe_296', 'fe_297', 'fe_298', 'fe_299', '339 max', '339 mean', '339 std', '68 max', '68 mean', '68 std', '215 max', '215 mean', '215 std', '503 max', '503 mean', '503 std', '251 max', '251 mean', '251 std', '316 max', '316 mean', '316 std', '314 max', '314 mean', '314 std', '418 max', '418 mean', '418 std', '284 max', '284 mean', '284 std', '289 max', '289 mean', '289 std', '415 max', '415 mean', '415 std', '765 max', '765 mean', '765 std', '165 max', '165 mean', '165 std', '556 max', '556 mean', '556 std', '391 max', '391 mean', '391 std', '535 max', '535 mean', '535 std', '385 max', '385 mean', '385 std', '147 max', '147 mean', '147 std', '25 max', '25 mean', '25 std', '526 max', '526 mean', '526 std', '40 max', '40 mean', '40 std', '199 max', '199 mean', '199 std', '583 max', '583 mean', '583 std', '273 max', '273 mean', '273 std', '169 max', '169 mean', '169 std', '15 max', '15 mean', '15 std', '592 max', '592 mean', '592 std', '268 max', '268 mean', '268 std', '437 max', '437 mean', '437 std', '315 max', '315 mean', '315 std', '893 max', '893 mean', '893 std', '493 max', '493 mean', '493 std', '465 max', '465 mean', '465 std', '1450 max', '1450 mean', '1450 std', '794 max', '794 mean', '794 std', '1819 max', '1819 mean', '1819 std', 'social_media max', 'social_media mean', 'social_media std', 'region_name_fq', 'city_name_fq', 'url_host_fq', 'region_name_un', 'city_name_un', 'cpe_type_cd_un', 'cpe_model_os_type_un', '339 sum', '68 sum', '215 sum', '503 sum', '251 sum', '316 sum', '314 sum', '418 sum', '284 sum', '289 sum', '415 sum', '765 sum', '165 sum', '556 sum', '391 sum', '535 sum', '385 sum', '147 sum', '25 sum', '526 sum', '40 sum', '199 sum', '583 sum', '273 sum', '169 sum', '15 sum', '592 sum', '268 sum', '437 sum', '315 sum', '893 sum', '493 sum', '465 sum', '1450 sum', '794 sum', '1819 sum', '0_u2', '1_u2', '2_u2', '3_u2', '4_u2', '5_u2', '6_u2', '7_u2', '8_u2', '9_u2', '10_u2', '11_u2', '12_u2', '13_u2', '14_u2', '15_u2', '16_u2', '17_u2', '18_u2', '19_u2', '20_u2', '21_u2', '22_u2', '23_u2', '24_u2', '25_u2', '26_u2', '27_u2', '28_u2', '29_u2', '30_u2', '31_u2', '32_u2', '33_u2', '34_u2', '35_u2', '36_u2', '37_u2', '38_u2', '39_u2', '40_u2', '41_u2', '42_u2', '43_u2', '44_u2', '45_u2', '46_u2', '47_u2', '48_u2', '49_u2', '50_u2', '51_u2', '52_u2', '53_u2', '54_u2', '55_u2', '56_u2', '57_u2', '58_u2', '59_u2', '60_u2', '61_u2', '62_u2', '63_u2', '64_u2', '65_u2', '66_u2', '67_u2', '68_u2', '69_u2', '70_u2', '71_u2', '72_u2', '73_u2', '74_u2', '75_u2', '76_u2', '77_u2', '78_u2', '79_u2', '80_u2', '81_u2', '82_u2', '83_u2', '84_u2', '85_u2', '86_u2', '87_u2', '88_u2', '89_u2', '90_u2', '91_u2', '92_u2', '93_u2', '94_u2', '95_u2', '96_u2', '97_u2', '98_u2', '99_u2', '100_u2', '101_u2', '102_u2', '103_u2', '104_u2', '105_u2', '106_u2', '107_u2', '108_u2', '109_u2', '110_u2', '111_u2', '112_u2', '113_u2', '114_u2', '115_u2', '116_u2', '117_u2', '118_u2', '119_u2', '120_u2', '121_u2', '122_u2', '123_u2', '124_u2', '125_u2', '126_u2', '127_u2', '128_u2', '129_u2', '130_u2', '131_u2', '132_u2', '133_u2', '134_u2', '135_u2', '136_u2', '137_u2', '138_u2', '139_u2', '140_u2', '141_u2', '142_u2', '143_u2', '144_u2', '145_u2', '146_u2', '147_u2', '148_u2', '149_u2', '150_u2', '151_u2', '152_u2', '153_u2', '154_u2', '155_u2', '156_u2', '157_u2', '158_u2', '159_u2', '160_u2', '161_u2', '162_u2', '163_u2', '164_u2', '165_u2', '166_u2', '167_u2', '168_u2', '169_u2', '170_u2', '171_u2', '172_u2', '173_u2', '174_u2', '175_u2', '176_u2', '177_u2', '178_u2', '179_u2', '180_u2', '181_u2', '182_u2', '183_u2', '184_u2', '185_u2', '186_u2', '187_u2', '188_u2', '189_u2', '190_u2', '191_u2', '192_u2', '193_u2', '194_u2', '195_u2', '196_u2', '197_u2', '198_u2', '199_u2', '200_u2', '201_u2', '202_u2', '203_u2', '204_u2', '205_u2', '206_u2', '207_u2', '208_u2', '209_u2', '210_u2', '211_u2', '212_u2', '213_u2', '214_u2', '215_u2', '216_u2', '217_u2', '218_u2', '219_u2', '220_u2', '221_u2', '222_u2', '223_u2', '224_u2', '225_u2', '226_u2', '227_u2', '228_u2', '229_u2', '230_u2', '231_u2', '232_u2', '233_u2', '234_u2', '235_u2', '236_u2', '237_u2', '238_u2', '239_u2', '240_u2', '241_u2', '242_u2', '243_u2', '244_u2', '245_u2', '246_u2', '247_u2', '248_u2', '249_u2', '250_u2', '251_u2', '252_u2', '253_u2', '254_u2', '255_u2', '256_u2', '257_u2', '258_u2', '259_u2', '260_u2', '261_u2', '262_u2', '263_u2', '264_u2', '265_u2', '266_u2', '267_u2', '268_u2', '269_u2', '270_u2', '271_u2', '272_u2', '273_u2', '274_u2', '275_u2', '276_u2', '277_u2', '278_u2', '279_u2', '280_u2', '281_u2', '282_u2', '283_u2', '284_u2', '285_u2', '286_u2', '287_u2', '288_u2', '289_u2', '290_u2', '291_u2', '292_u2', '293_u2', '294_u2', '295_u2', '296_u2', '297_u2', '298_u2', '299_u2', '300_u2', '301_u2', '302_u2', '303_u2', '304_u2', '305_u2', '306_u2', '307_u2', '308_u2', '309_u2', '310_u2', '311_u2', '312_u2', '313_u2', '314_u2', '315_u2', '316_u2', '317_u2', '318_u2', '319_u2', '320_u2', '321_u2', '322_u2', '323_u2', '324_u2', '325_u2', '326_u2', '327_u2', '328_u2', '329_u2', '330_u2', '331_u2', '332_u2', '333_u2', '334_u2', '335_u2', '336_u2', '337_u2', '338_u2', '339_u2', '340_u2', '341_u2', '342_u2', '343_u2', '344_u2', '345_u2', '346_u2', '347_u2', '348_u2', '349_u2', '350_u2', '351_u2', '352_u2', '353_u2', '354_u2', '355_u2', '356_u2', '357_u2', '358_u2', '359_u2', '360_u2', '361_u2', '362_u2', '363_u2', '364_u2', '365_u2', '366_u2', '367_u2', '368_u2', '369_u2', '370_u2', '371_u2', '372_u2', '373_u2', '374_u2', '375_u2', '376_u2', '377_u2', '378_u2', '379_u2', '380_u2', '381_u2', '382_u2', '383_u2', '384_u2', '385_u2', '386_u2', '387_u2', '388_u2', '389_u2', '390_u2', '391_u2', '392_u2', '393_u2', '394_u2', '395_u2', '396_u2', '397_u2', '398_u2', '399_u2', '400_u2', '401_u2', '402_u2', '403_u2', '404_u2', '405_u2', '406_u2', '407_u2', '408_u2', '409_u2', '410_u2', '411_u2', '412_u2', '413_u2', '414_u2', '415_u2', '416_u2', '417_u2', '418_u2', '419_u2', '420_u2', '421_u2', '422_u2', '423_u2', '424_u2', '425_u2', '426_u2', '427_u2', '428_u2', '429_u2', '430_u2', '431_u2', '432_u2', '433_u2', '434_u2', '435_u2', '436_u2', '437_u2', '438_u2', '439_u2', '440_u2', '441_u2', '442_u2', '443_u2', '444_u2', '445_u2', '446_u2', '447_u2', '448_u2', '449_u2', '450_u2', '451_u2', '452_u2', '453_u2', '454_u2', '455_u2', '456_u2', '457_u2', '458_u2', '459_u2', '460_u2', '461_u2', '462_u2', '463_u2', '464_u2', '465_u2', '466_u2', '467_u2', '468_u2', '469_u2', '470_u2', '471_u2', '472_u2', '473_u2', '474_u2', '475_u2', '476_u2', '477_u2', '478_u2', '479_u2', '480_u2', '481_u2', '482_u2', '483_u2', '484_u2', '485_u2', '486_u2', '487_u2', '488_u2', '489_u2', '490_u2', '491_u2', '492_u2', '493_u2', '494_u2', '495_u2', '496_u2', '497_u2', '498_u2', '499_u2', '0_u3', '1_u3', '2_u3', '3_u3', '4_u3', '5_u3', '6_u3', '7_u3', '8_u3', '9_u3', '10_u3', '11_u3', '12_u3', '13_u3', '14_u3', '15_u3', '16_u3', '17_u3', '18_u3', '19_u3', '20_u3', '21_u3', '22_u3', '23_u3', '24_u3', '25_u3', '26_u3', '27_u3', '28_u3', '29_u3', '30_u3', '31_u3', '32_u3', '33_u3', '34_u3', '35_u3', '36_u3', '37_u3', '38_u3', '39_u3', '40_u3', '41_u3', '42_u3', '43_u3', '44_u3', '45_u3', '46_u3', '47_u3', '48_u3', '49_u3', '50_u3', '51_u3', '52_u3', '53_u3', '54_u3', '55_u3', '56_u3', '57_u3', '58_u3', '59_u3', '60_u3', '61_u3', '62_u3', '63_u3', '64_u3', '65_u3', '66_u3', '67_u3', '68_u3', '69_u3', '70_u3', '71_u3', '72_u3', '73_u3', '74_u3', '75_u3', '76_u3', '77_u3', '78_u3', '79_u3', '80_u3', '81_u3', '82_u3', '83_u3', '84_u3', '85_u3', '86_u3', '87_u3', '88_u3', '89_u3', '90_u3', '91_u3', '92_u3', '93_u3', '94_u3', '95_u3', '96_u3', '97_u3', '98_u3', '99_u3', '100_u3', '101_u3', '102_u3', '103_u3', '104_u3', '105_u3', '106_u3', '107_u3', '108_u3', '109_u3', '110_u3', '111_u3', '112_u3', '113_u3', '114_u3', '115_u3', '116_u3', '117_u3', '118_u3', '119_u3', '120_u3', '121_u3', '122_u3', '123_u3', '124_u3', '125_u3', '126_u3', '127_u3', '128_u3', '129_u3', '130_u3', '131_u3', '132_u3', '133_u3', '134_u3', '135_u3', '136_u3', '137_u3', '138_u3', '139_u3', '140_u3', '141_u3', '142_u3', '143_u3', '144_u3', '145_u3', '146_u3', '147_u3', '148_u3', '149_u3', '150_u3', '151_u3', '152_u3', '153_u3', '154_u3', '155_u3', '156_u3', '157_u3', '158_u3', '159_u3', '160_u3', '161_u3', '162_u3', '163_u3', '164_u3', '165_u3', '166_u3', '167_u3', '168_u3', '169_u3', '170_u3', '171_u3', '172_u3', '173_u3', '174_u3', '175_u3', '176_u3', '177_u3', '178_u3', '179_u3', '180_u3', '181_u3', '182_u3', '183_u3', '184_u3', '185_u3', '186_u3', '187_u3', '188_u3', '189_u3', '190_u3', '191_u3', '192_u3', '193_u3', '194_u3', '195_u3', '196_u3', '197_u3', '198_u3', '199_u3', '200_u3', '201_u3', '202_u3', '203_u3', '204_u3', '205_u3', '206_u3', '207_u3', '208_u3', '209_u3', '210_u3', '211_u3', '212_u3', '213_u3', '214_u3', '215_u3', '216_u3', '217_u3', '218_u3', '219_u3', '220_u3', '221_u3', '222_u3', '223_u3', '224_u3', '225_u3', '226_u3', '227_u3', '228_u3', '229_u3', '230_u3', '231_u3', '232_u3', '233_u3', '234_u3', '235_u3', '236_u3', '237_u3', '238_u3', '239_u3', '240_u3', '241_u3', '242_u3', '243_u3', '244_u3', '245_u3', '246_u3', '247_u3', '248_u3', '249_u3', '250_u3', '251_u3', '252_u3', '253_u3', '254_u3', '255_u3', '256_u3', '257_u3', '258_u3', '259_u3', '260_u3', '261_u3', '262_u3', '263_u3', '264_u3', '265_u3', '266_u3', '267_u3', '268_u3', '269_u3', '270_u3', '271_u3', '272_u3', '273_u3', '274_u3', '275_u3', '276_u3', '277_u3', '278_u3', '279_u3', '280_u3', '281_u3', '282_u3', '283_u3', '284_u3', '285_u3', '286_u3', '287_u3', '288_u3', '289_u3', '290_u3', '291_u3', '292_u3', '293_u3', '294_u3', '295_u3', '296_u3', '297_u3', '298_u3', '299_u3', '300_u3', '301_u3', '302_u3', '303_u3', '304_u3', '305_u3', '306_u3', '307_u3', '308_u3', '309_u3', '310_u3', '311_u3', '312_u3', '313_u3', '314_u3', '315_u3', '316_u3', '317_u3', '318_u3', '319_u3', '320_u3', '321_u3', '322_u3', '323_u3', '324_u3', '325_u3', '326_u3', '327_u3', '328_u3', '329_u3', '330_u3', '331_u3', '332_u3', '333_u3', '334_u3', '335_u3', '336_u3', '337_u3', '338_u3', '339_u3', '340_u3', '341_u3', '342_u3', '343_u3', '344_u3', '345_u3', '346_u3', '347_u3', '348_u3', '349_u3', '350_u3', '351_u3', '352_u3', '353_u3', '354_u3', '355_u3', '356_u3', '357_u3', '358_u3', '359_u3', '360_u3', '361_u3', '362_u3', '363_u3', '364_u3', '365_u3', '366_u3', '367_u3', '368_u3', '369_u3', '370_u3', '371_u3', '372_u3', '373_u3', '374_u3', '375_u3', '376_u3', '377_u3', '378_u3', '379_u3', '380_u3', '381_u3', '382_u3', '383_u3', '384_u3', '385_u3', '386_u3', '387_u3', '388_u3', '389_u3', '390_u3', '391_u3', '392_u3', '393_u3', '394_u3', '395_u3', '396_u3', '397_u3', '398_u3', '399_u3', '400_u3', '401_u3', '402_u3', '403_u3', '404_u3', '405_u3', '406_u3', '407_u3', '408_u3', '409_u3', '410_u3', '411_u3', '412_u3', '413_u3', '414_u3', '415_u3', '416_u3', '417_u3', '418_u3', '419_u3', '420_u3', '421_u3', '422_u3', '423_u3', '424_u3', '425_u3', '426_u3', '427_u3', '428_u3', '429_u3', '430_u3', '431_u3', '432_u3', '433_u3', '434_u3', '435_u3', '436_u3', '437_u3', '438_u3', '439_u3', '440_u3', '441_u3', '442_u3', '443_u3', '444_u3', '445_u3', '446_u3', '447_u3', '448_u3', '449_u3', '450_u3', '451_u3', '452_u3', '453_u3', '454_u3', '455_u3', '456_u3', '457_u3', '458_u3', '459_u3', '460_u3', '461_u3', '462_u3', '463_u3', '464_u3', '465_u3', '466_u3', '467_u3', '468_u3', '469_u3', '470_u3', '471_u3', '472_u3', '473_u3', '474_u3', '475_u3', '476_u3', '477_u3', '478_u3', '479_u3', '480_u3', '481_u3', '482_u3', '483_u3', '484_u3', '485_u3', '486_u3', '487_u3', '488_u3', '489_u3', '490_u3', '491_u3', '492_u3', '493_u3', '494_u3', '495_u3', '496_u3', '497_u3', '498_u3', '499_u3', '500_u3', '501_u3', '502_u3', '503_u3', '504_u3', '505_u3', '506_u3', '507_u3', '508_u3', '509_u3', '510_u3', '511_u3', '512_u3', '513_u3', '514_u3', '515_u3', '516_u3', '517_u3', '518_u3', '519_u3', '520_u3', '521_u3', '522_u3', '523_u3', '524_u3', '525_u3', '526_u3', '527_u3', '528_u3', '529_u3', '530_u3', '531_u3', '532_u3', '533_u3', '534_u3', '535_u3', '536_u3', '537_u3', '538_u3', '539_u3', '540_u3', '541_u3', '542_u3', '543_u3', '544_u3', '545_u3', '546_u3', '547_u3', '548_u3', '549_u3', '550_u3', '551_u3', '552_u3', '553_u3', '554_u3', '555_u3', '556_u3', '557_u3', '558_u3', '559_u3', '560_u3', '561_u3', '562_u3', '563_u3', '564_u3', '565_u3', '566_u3', '567_u3', '568_u3', '569_u3', '570_u3', '571_u3', '572_u3', '573_u3', '574_u3', '575_u3', '576_u3', '577_u3', '578_u3', '579_u3', '580_u3', '581_u3', '582_u3', '583_u3', '584_u3', '585_u3', '586_u3', '587_u3', '588_u3', '589_u3', '590_u3', '591_u3', '592_u3', '593_u3', '594_u3', '595_u3', '596_u3', '597_u3', '598_u3', '599_u3', '600_u3', '601_u3', '602_u3', '603_u3', '604_u3', '605_u3', '606_u3', '607_u3', '608_u3', '609_u3', '610_u3', '611_u3', '612_u3', '613_u3', '614_u3', '615_u3', '616_u3', '617_u3', '618_u3', '619_u3', '620_u3', '621_u3', '622_u3', '623_u3', '624_u3', '625_u3', '626_u3', '627_u3', '628_u3', '629_u3', '630_u3', '631_u3', '632_u3', '633_u3', '634_u3', '635_u3', '636_u3', '637_u3', '638_u3', '639_u3', '640_u3', '641_u3', '642_u3', '643_u3', '644_u3', '645_u3', '646_u3', '647_u3', '648_u3', '649_u3', '650_u3', '651_u3', '652_u3', '653_u3', '654_u3', '655_u3', '656_u3', '657_u3', '658_u3', '659_u3', '660_u3', '661_u3', '662_u3', '663_u3', '664_u3', '665_u3', '666_u3', '667_u3', '668_u3', '669_u3', '670_u3', '671_u3', '672_u3', '673_u3', '674_u3', '675_u3', '676_u3', '677_u3', '678_u3', '679_u3', '680_u3', '681_u3', '682_u3', '683_u3', '684_u3', '685_u3', '686_u3', '687_u3', '688_u3', '689_u3', '690_u3', '691_u3', '692_u3', '693_u3', '694_u3', '695_u3', '696_u3', '697_u3', '698_u3', '699_u3', '700_u3', '701_u3', '702_u3', '703_u3', '704_u3', '705_u3', '706_u3', '707_u3', '708_u3', '709_u3', '710_u3', '711_u3', '712_u3', '713_u3', '714_u3', '715_u3', '716_u3', '717_u3', '718_u3', '719_u3', '720_u3', '721_u3', '722_u3', '723_u3', '724_u3', '725_u3', '726_u3', '727_u3', '728_u3', '729_u3', '730_u3', '731_u3', '732_u3', '733_u3', '734_u3', '735_u3', '736_u3', '737_u3', '738_u3', '739_u3', '740_u3', '741_u3', '742_u3', '743_u3', '744_u3', '745_u3', '746_u3', '747_u3', '748_u3', '749_u3', '750_u3', '751_u3', '752_u3', '753_u3', '754_u3', '755_u3', '756_u3', '757_u3', '758_u3', '759_u3', '760_u3', '761_u3', '762_u3', '763_u3', '764_u3', '765_u3', '766_u3', '767_u3', '768_u3', '769_u3', '770_u3', '771_u3', '772_u3', '773_u3', '774_u3', '775_u3', '776_u3', '777_u3', '778_u3', '779_u3', '780_u3', '781_u3', '782_u3', '783_u3', '784_u3', '785_u3', '786_u3', '787_u3', '788_u3', '789_u3', '790_u3', '791_u3', '792_u3', '793_u3', '794_u3', '795_u3', '796_u3', '797_u3', '798_u3', '799_u3', '800_u3', '801_u3', '802_u3', '803_u3', '804_u3', '805_u3', '806_u3', '807_u3', '808_u3', '809_u3', '810_u3', '811_u3', '812_u3', '813_u3', '814_u3', '815_u3', '816_u3', '817_u3', '818_u3', '819_u3', '820_u3', '821_u3', '822_u3', '823_u3', '824_u3', '825_u3', '826_u3', '827_u3', '828_u3', '829_u3', '830_u3', '831_u3', '832_u3', '833_u3', '834_u3', '835_u3', '836_u3', '837_u3', '838_u3', '839_u3', '840_u3', '841_u3', '842_u3', '843_u3', '844_u3', '845_u3', '846_u3', '847_u3', '848_u3', '849_u3', '850_u3', '851_u3', '852_u3', '853_u3', '854_u3', '855_u3', '856_u3', '857_u3', '858_u3', '859_u3', '860_u3', '861_u3', '862_u3', '863_u3', '864_u3', '865_u3', '866_u3', '867_u3', '868_u3', '869_u3', '870_u3', '871_u3', '872_u3', '873_u3', '874_u3', '875_u3', '876_u3', '877_u3', '878_u3', '879_u3', '880_u3', '881_u3', '882_u3', '883_u3', '884_u3', '885_u3', '886_u3', '887_u3', '888_u3', '889_u3', '890_u3', '891_u3', '892_u3', '893_u3', '894_u3', '895_u3', '896_u3', '897_u3', '898_u3', '899_u3', '900_u3', '901_u3', '902_u3', '903_u3', '904_u3', '905_u3', '906_u3', '907_u3', '908_u3', '909_u3', '910_u3', '911_u3', '912_u3', '913_u3', '914_u3', '915_u3', '916_u3', '917_u3', '918_u3', '919_u3', '920_u3', '921_u3', '922_u3', '923_u3', '924_u3', '925_u3', '926_u3', '927_u3', '928_u3', '929_u3', '930_u3', '931_u3', '932_u3', '933_u3', '934_u3', '935_u3', '936_u3', '937_u3', '938_u3', '939_u3', '940_u3', '941_u3', '942_u3', '943_u3', '944_u3', '945_u3', '946_u3', '947_u3', '948_u3', '949_u3', '950_u3', '951_u3', '952_u3', '953_u3', '954_u3', '955_u3', '956_u3', '957_u3', '958_u3', '959_u3', '960_u3', '961_u3', '962_u3', '963_u3', '964_u3', '965_u3', '966_u3', '967_u3', '968_u3', '969_u3', '970_u3', '971_u3', '972_u3', '973_u3', '974_u3', '975_u3', '976_u3', '977_u3', '978_u3', '979_u3', '980_u3', '981_u3', '982_u3', '983_u3', '984_u3', '985_u3', '986_u3', '987_u3', '988_u3', '989_u3', '990_u3', '991_u3', '992_u3', '993_u3', '994_u3', '995_u3', '996_u3', '997_u3', '998_u3', '999_u3', '1000_u3', '1001_u3', '1002_u3', '1003_u3', '1004_u3', '1005_u3', '1006_u3', '1007_u3', '1008_u3', '1009_u3', '1010_u3', '1011_u3', '1012_u3', '1013_u3', '1014_u3', '1015_u3', '1016_u3', '1017_u3', '1018_u3', '1019_u3', '1020_u3', '1021_u3', '1022_u3', '1023_u3', '1024_u3', '1025_u3', '1026_u3', '1027_u3', '1028_u3', '1029_u3', '1030_u3', '1031_u3', '1032_u3', '1033_u3', '1034_u3', '1035_u3', '1036_u3', '1037_u3', '1038_u3', '1039_u3', '1040_u3', '1041_u3', '1042_u3', '1043_u3', '1044_u3', '1045_u3', '1046_u3', '1047_u3', '1048_u3', '1049_u3', '1050_u3', '1051_u3', '1052_u3', '1053_u3', '1054_u3', '1055_u3', '1056_u3', '1057_u3', '1058_u3', '1059_u3', '1060_u3', '1061_u3', '1062_u3', '1063_u3', '1064_u3', '1065_u3', '1066_u3', '1067_u3', '1068_u3', '1069_u3', '1070_u3', '1071_u3', '1072_u3', '1073_u3', '1074_u3', '1075_u3', '1076_u3', '1077_u3', '1078_u3', '1079_u3', '1080_u3', '1081_u3', '1082_u3', '1083_u3', '1084_u3', '1085_u3', '1086_u3', '1087_u3', '1088_u3', '1089_u3', '1090_u3', '1091_u3', '1092_u3', '1093_u3', '1094_u3', '1095_u3', '1096_u3', '1097_u3', '1098_u3', '1099_u3', '1100_u3', '1101_u3', '1102_u3', '1103_u3', '1104_u3', '1105_u3', '1106_u3', '1107_u3', '1108_u3', '1109_u3', '1110_u3', '1111_u3', '1112_u3', '1113_u3', '1114_u3', '1115_u3', '1116_u3', '1117_u3', '1118_u3', '1119_u3', '1120_u3', '1121_u3', '1122_u3', '1123_u3', '1124_u3', '1125_u3', '1126_u3', '1127_u3', '1128_u3', '1129_u3', '1130_u3', '1131_u3', '1132_u3', '1133_u3', '1134_u3', '1135_u3', '1136_u3', '1137_u3', '1138_u3', '1139_u3', '1140_u3', '1141_u3', '1142_u3', '1143_u3', '1144_u3', '1145_u3', '1146_u3', '1147_u3', '1148_u3', '1149_u3', '1150_u3', '1151_u3', '1152_u3', '1153_u3', '1154_u3', '1155_u3', '1156_u3', '1157_u3', '1158_u3', '1159_u3', '1160_u3', '1161_u3', '1162_u3', '1163_u3', '1164_u3', '1165_u3', '1166_u3', '1167_u3', '1168_u3', '1169_u3', '1170_u3', '1171_u3', '1172_u3', '1173_u3', '1174_u3', '1175_u3', '1176_u3', '1177_u3', '1178_u3', '1179_u3', '1180_u3', '1181_u3', '1182_u3', '1183_u3', '1184_u3', '1185_u3', '1186_u3', '1187_u3', '1188_u3', '1189_u3', '1190_u3', '1191_u3', '1192_u3', '1193_u3', '1194_u3', '1195_u3', '1196_u3', '1197_u3', '1198_u3', '1199_u3', '1200_u3', '1201_u3', '1202_u3', '1203_u3', '1204_u3', '1205_u3', '1206_u3', '1207_u3', '1208_u3', '1209_u3', '1210_u3', '1211_u3', '1212_u3', '1213_u3', '1214_u3', '1215_u3', '1216_u3', '1217_u3', '1218_u3', '1219_u3', '1220_u3', '1221_u3', '1222_u3', '1223_u3', '1224_u3', '1225_u3', '1226_u3', '1227_u3', '1228_u3', '1229_u3', '1230_u3', '1231_u3', '1232_u3', '1233_u3', '1234_u3', '1235_u3', '1236_u3', '1237_u3', '1238_u3', '1239_u3', '1240_u3', '1241_u3', '1242_u3', '1243_u3', '1244_u3', '1245_u3', '1246_u3', '1247_u3', '1248_u3', '1249_u3', '1250_u3', '1251_u3', '1252_u3', '1253_u3', '1254_u3', '1255_u3', '1256_u3', '1257_u3', '1258_u3', '1259_u3', '1260_u3', '1261_u3', '1262_u3', '1263_u3', '1264_u3', '1265_u3', '1266_u3', '1267_u3', '1268_u3', '1269_u3', '1270_u3', '1271_u3', '1272_u3', '1273_u3', '1274_u3', '1275_u3', '1276_u3', '1277_u3', '1278_u3', '1279_u3', '1280_u3', '1281_u3', '1282_u3', '1283_u3', '1284_u3', '1285_u3', '1286_u3', '1287_u3', '1288_u3', '1289_u3', '1290_u3', '1291_u3', '1292_u3', '1293_u3', '1294_u3', '1295_u3', '1296_u3', '1297_u3', '1298_u3', '1299_u3', '1300_u3', '1301_u3', '1302_u3', '1303_u3', '1304_u3', '1305_u3', '1306_u3', '1307_u3', '1308_u3', '1309_u3', '1310_u3', '1311_u3', '1312_u3', '1313_u3', '1314_u3', '1315_u3', '1316_u3', '1317_u3', '1318_u3', '1319_u3', '1320_u3', '1321_u3', '1322_u3', '1323_u3', '1324_u3', '1325_u3', '1326_u3', '1327_u3', '1328_u3', '1329_u3', '1330_u3', '1331_u3', '1332_u3', '1333_u3', '1334_u3', '1335_u3', '1336_u3', '1337_u3', '1338_u3', '1339_u3', '1340_u3', '1341_u3', '1342_u3', '1343_u3', '1344_u3', '1345_u3', '1346_u3', '1347_u3', '1348_u3', '1349_u3', '1350_u3', '1351_u3', '1352_u3', '1353_u3', '1354_u3', '1355_u3', '1356_u3', '1357_u3', '1358_u3', '1359_u3', '1360_u3', '1361_u3', '1362_u3', '1363_u3', '1364_u3', '1365_u3', '1366_u3', '1367_u3', '1368_u3', '1369_u3', '1370_u3', '1371_u3', '1372_u3', '1373_u3', '1374_u3', '1375_u3', '1376_u3', '1377_u3', '1378_u3', '1379_u3', '1380_u3', '1381_u3', '1382_u3', '1383_u3', '1384_u3', '1385_u3', '1386_u3', '1387_u3', '1388_u3', '1389_u3', '1390_u3', '1391_u3', '1392_u3', '1393_u3', '1394_u3', '1395_u3', '1396_u3', '1397_u3', '1398_u3', '1399_u3', '1400_u3', '1401_u3', '1402_u3', '1403_u3', '1404_u3', '1405_u3', '1406_u3', '1407_u3', '1408_u3', '1409_u3', '1410_u3', '1411_u3', '1412_u3', '1413_u3', '1414_u3', '1415_u3', '1416_u3', '1417_u3', '1418_u3', '1419_u3', '1420_u3', '1421_u3', '1422_u3', '1423_u3', '1424_u3', '1425_u3', '1426_u3', '1427_u3', '1428_u3', '1429_u3', '1430_u3', '1431_u3', '1432_u3', '1433_u3', '1434_u3', '1435_u3', '1436_u3', '1437_u3', '1438_u3', '1439_u3', '1440_u3', '1441_u3', '1442_u3', '1443_u3', '1444_u3', '1445_u3', '1446_u3', '1447_u3', '1448_u3', '1449_u3', '1450_u3', '1451_u3', '1452_u3', '1453_u3', '1454_u3', '1455_u3', '1456_u3', '1457_u3', '1458_u3', '1459_u3', '1460_u3', '1461_u3', '1462_u3', '1463_u3', '1464_u3', '1465_u3', '1466_u3', '1467_u3', '1468_u3', '1469_u3', '1470_u3', '1471_u3', '1472_u3', '1473_u3', '1474_u3', '1475_u3', '1476_u3', '1477_u3', '1478_u3', '1479_u3', '1480_u3', '1481_u3', '1482_u3', '1483_u3', '1484_u3', '1485_u3', '1486_u3', '1487_u3', '1488_u3', '1489_u3', '1490_u3', '1491_u3', '1492_u3', '1493_u3', '1494_u3', '1495_u3', '1496_u3', '1497_u3', '1498_u3', '1499_u3','als_emb_url_host_rc_0', 'als_emb_url_host_rc_1', 'als_emb_url_host_rc_2', 'als_emb_url_host_rc_3', 'als_emb_url_host_rc_4', 'als_emb_url_host_rc_5', 'als_emb_url_host_rc_6', 'als_emb_url_host_rc_7', 'als_emb_url_host_rc_8', 'als_emb_url_host_rc_9', 'als_emb_url_host_rc_10', 'als_emb_url_host_rc_11', 'als_emb_url_host_rc_12', 'als_emb_url_host_rc_13', 'als_emb_url_host_rc_14', 'als_emb_url_host_rc_15', 'als_emb_url_host_rc_16', 'als_emb_url_host_rc_17', 'als_emb_url_host_rc_18', 'als_emb_url_host_rc_19', 'als_emb_url_host_rc_20', 'als_emb_url_host_rc_21', 'als_emb_url_host_rc_22', 'als_emb_url_host_rc_23', 'als_emb_url_host_rc_24', 'als_emb_url_host_rc_25', 'als_emb_url_host_rc_26', 'als_emb_url_host_rc_27', 'als_emb_url_host_rc_28', 'als_emb_url_host_rc_29', 'als_emb_url_host_rc_30', 'als_emb_url_host_rc_31', 'als_emb_url_host_wh0', 'als_emb_url_host_wh1', 'als_emb_url_host_wh2', 'als_emb_url_host_wh3', 'als_emb_url_host_wh4', 'als_emb_url_host_wh5', 'als_emb_url_host_wh6', 'als_emb_url_host_wh7', 'als_emb_url_host_wh8', 'als_emb_url_host_wh9', 'als_emb_url_host_wh10', 'als_emb_url_host_wh11', 'als_emb_url_host_wh12', 'als_emb_url_host_wh13', 'als_emb_url_host_wh14', 'als_emb_url_host_wh15', 'als_emb_url_host_wh16', 'als_emb_url_host_wh17', 'als_emb_url_host_wh18', 'als_emb_url_host_wh19', 'als_emb_url_host_wh20', 'als_emb_url_host_wh21', 'als_emb_url_host_wh22', 'als_emb_url_host_wh23', 'als_emb_url_host_wh24', 'als_emb_url_host_wh25', 'als_emb_url_host_wh26', 'als_emb_url_host_wh27', 'als_emb_url_host_wh28', 'als_emb_url_host_wh29', 'als_emb_url_host_wh30', 'als_emb_url_host_wh31', 'als_emb_url_host_wh32', 'als_emb_url_host_wh33', 'als_emb_url_host_wh34', 'als_emb_url_host_wh35', 'als_emb_url_host_wh36', 'als_emb_url_host_wh37', 'als_emb_url_host_wh38', 'als_emb_url_host_wh39', 'als_emb_url_host_wh40', 'als_emb_url_host_wh41', 'als_emb_url_host_wh42', 'als_emb_url_host_wh43', 'als_emb_url_host_wh44', 'als_emb_url_host_wh45', 'als_emb_url_host_wh46', 'als_emb_url_host_wh47', 'als_emb_url_host_wh48', 'als_emb_url_host_wh49', 'als_emb_url_host_wh50', 'als_emb_url_host_wh51', 'als_emb_url_host_wh52', 'als_emb_url_host_wh53', 'als_emb_url_host_wh54', 'als_emb_url_host_wh55', 'als_emb_url_host_wh56', 'als_emb_url_host_wh57', 'als_emb_url_host_wh58', 'als_emb_url_host_wh59', 'als_emb_url_host_wh60', 'als_emb_url_host_wh61', 'als_emb_url_host_wh62', 'als_emb_url_host_wh63', 'als_emb_url_host_um0', 'als_emb_url_host_um1', 'als_emb_url_host_um2', 'als_emb_url_host_um3', 'als_emb_url_host_um4', 'als_emb_url_host_um5', 'als_emb_url_host_um6', 'als_emb_url_host_um7', 'als_emb_url_host_um8', 'als_emb_url_host_um9', 'als_emb_url_host_um10', 'als_emb_url_host_um11', 'als_emb_url_host_um12', 'als_emb_url_host_um13', 'als_emb_url_host_um14', 'als_emb_url_host_um15', 'als_emb_url_host_um16', 'als_emb_url_host_um17', 'als_emb_url_host_um18', 'als_emb_url_host_um19', 'als_emb_url_host_um20', 'als_emb_url_host_um21', 'als_emb_url_host_um22', 'als_emb_url_host_um23', 'als_emb_url_host_um24', 'als_emb_url_host_um25', 'als_emb_url_host_um26', 'als_emb_url_host_um27', 'als_emb_url_host_um28', 'als_emb_url_host_um29', 'als_emb_url_host_um30', 'als_emb_url_host_um31', 'als_emb_url_host_um32', 'als_emb_url_host_um33', 'als_emb_url_host_um34', 'als_emb_url_host_um35', 'als_emb_url_host_um36', 'als_emb_url_host_um37', 'als_emb_url_host_um38', 'als_emb_url_host_um39', 'als_emb_url_host_um40', 'als_emb_url_host_um41', 'als_emb_url_host_um42', 'als_emb_url_host_um43', 'als_emb_url_host_um44', 'als_emb_url_host_um45', 'als_emb_url_host_um46', 'als_emb_url_host_um47', 'als_emb_url_host_um48', 'als_emb_url_host_um49', 'als_emb_url_host_um50', 'als_emb_url_host_um51', 'als_emb_url_host_um52', 'als_emb_url_host_um53', 'als_emb_url_host_um54', 'als_emb_url_host_um55', 'als_emb_url_host_um56', 'als_emb_url_host_um57', 'als_emb_url_host_um58', 'als_emb_url_host_um59', 'als_emb_url_host_um60', 'als_emb_url_host_um61', 'als_emb_url_host_um62', 'als_emb_url_host_um63', 'als_emb_url_host_um64', 'als_emb_url_host_um65', 'als_emb_url_host_um66', 'als_emb_url_host_um67', 'als_emb_url_host_um68', 'als_emb_url_host_um69', 'als_emb_url_host_um70', 'als_emb_url_host_um71', 'als_emb_url_host_um72', 'als_emb_url_host_um73', 'als_emb_url_host_um74', 'als_emb_url_host_um75', 'als_emb_url_host_um76', 'als_emb_url_host_um77', 'als_emb_url_host_um78', 'als_emb_url_host_um79', 'als_emb_url_host_um80', 'als_emb_url_host_um81', 'als_emb_url_host_um82', 'als_emb_url_host_um83', 'als_emb_url_host_um84', 'als_emb_url_host_um85', 'als_emb_url_host_um86', 'als_emb_url_host_um87', 'als_emb_url_host_um88', 'als_emb_url_host_um89', 'als_emb_url_host_um90', 'als_emb_url_host_um91', 'als_emb_url_host_um92', 'als_emb_url_host_um93', 'als_emb_url_host_um94', 'als_emb_url_host_um95', 'als_emb_url_host_um96', 'als_emb_url_host_um97', 'als_emb_url_host_um98', 'als_emb_url_host_um99', 'als_emb_url_host_um100', 'als_emb_url_host_um101', 'als_emb_url_host_um102', 'als_emb_url_host_um103', 'als_emb_url_host_um104', 'als_emb_url_host_um105', 'als_emb_url_host_um106', 'als_emb_url_host_um107', 'als_emb_url_host_um108', 'als_emb_url_host_um109', 'als_emb_url_host_um110', 'als_emb_url_host_um111', 'als_emb_url_host_um112', 'als_emb_url_host_um113', 'als_emb_url_host_um114', 'als_emb_url_host_um115', 'als_emb_url_host_um116', 'als_emb_url_host_um117', 'als_emb_url_host_um118', 'als_emb_url_host_um119', 'als_emb_url_host_um120', 'als_emb_url_host_um121', 'als_emb_url_host_um122', 'als_emb_url_host_um123', 'als_emb_url_host_um124', 'als_emb_url_host_um125', 'als_emb_url_host_um126', 'als_emb_url_host_um127', 'als_emb_url_host_dc_0', 'als_emb_url_host_dc_1', 'als_emb_url_host_dc_2', 'als_emb_url_host_dc_3', 'als_emb_url_host_dc_4', 'als_emb_url_host_dc_5', 'als_emb_url_host_dc_6', 'als_emb_url_host_dc_7', 'als_emb_url_host_dc_8', 'als_emb_url_host_dc_9', 'als_emb_url_host_dc_10', 'als_emb_url_host_dc_11', 'als_emb_url_host_dc_12', 'als_emb_url_host_dc_13', 'als_emb_url_host_dc_14', 'als_emb_url_host_dc_15', 'als_emb_url_host_dc_16', 'als_emb_url_host_dc_17', 'als_emb_url_host_dc_18', 'als_emb_url_host_dc_19', 'als_emb_url_host_dc_20', 'als_emb_url_host_dc_21', 'als_emb_url_host_dc_22', 'als_emb_url_host_dc_23', 'als_emb_url_host_dc_24', 'als_emb_url_host_dc_25', 'als_emb_url_host_dc_26', 'als_emb_url_host_dc_27', 'als_emb_url_host_dc_28', 'als_emb_url_host_dc_29', 'als_emb_url_host_dc_30', 'als_emb_url_host_dc_31', 'als_emb_url_host_dc_32', 'als_emb_url_host_dc_33', 'als_emb_url_host_dc_34', 'als_emb_url_host_dc_35', 'als_emb_url_host_dc_36', 'als_emb_url_host_dc_37', 'als_emb_url_host_dc_38', 'als_emb_url_host_dc_39', 'als_emb_url_host_dc_40', 'als_emb_url_host_dc_41', 'als_emb_url_host_dc_42', 'als_emb_url_host_dc_43', 'als_emb_url_host_dc_44', 'als_emb_url_host_dc_45', 'als_emb_url_host_dc_46', 'als_emb_url_host_dc_47', 'als_emb_url_host_dc_48', 'als_emb_url_host_dc_49', 'als_emb_url_host_dc_50', 'als_emb_url_host_dc_51', 'als_emb_url_host_dc_52', 'als_emb_url_host_dc_53', 'als_emb_url_host_dc_54', 'als_emb_url_host_dc_55', 'als_emb_url_host_dc_56', 'als_emb_url_host_dc_57', 'als_emb_url_host_dc_58', 'als_emb_url_host_dc_59', 'als_emb_url_host_dc_60', 'als_emb_url_host_dc_61', 'als_emb_url_host_dc_62', 'als_emb_url_host_dc_63', 'url_host_dc_clusters_age', 'url_host_dc_clusters_ismale']\n",
    "\n",
    "with open('cold_users.pickle', 'rb') as handle:\n",
    "    cold_users = pickle.load(handle)\n",
    "\n",
    "if cold:\n",
    "    train = train.drop(columns=warm_features)\n",
    "    test = test.drop(columns=warm_features)\n",
    "else:\n",
    "    train = train[~train.user_id.isin(cold_users)]\n",
    "    \n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257535\n"
     ]
    }
   ],
   "source": [
    "train = train[~train['is_male'].isna() & (train['is_male'].apply(lambda x: x != 'NA'))]\n",
    "train['is_male'] = train['is_male'].astype('int')\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import cross_entropy, mse_loss\n",
    "\n",
    "\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sparsemax(input, self.dim)\n",
    "\n",
    "\n",
    "class Entmax15Function(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        ctx.dim = dim\n",
    "\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input = input - max_val  # same numerical stability trick as for softmax\n",
    "        input = input / 2  # divide by 2 to solve actual Entmax\n",
    "\n",
    "        tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n",
    "        output = torch.clamp(input - tau_star, min=0) ** 2\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        Y, = ctx.saved_tensors\n",
    "        gppr = Y.sqrt()  # = 1 / g'' (Y)\n",
    "        dX = grad_output * gppr\n",
    "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
    "        q = q.unsqueeze(ctx.dim)\n",
    "        dX -= q * gppr\n",
    "        return dX, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "\n",
    "        rho = _make_ix_like(input, dim)\n",
    "        mean = Xsrt.cumsum(dim) / rho\n",
    "        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n",
    "        ss = rho * (mean_sq - mean ** 2)\n",
    "        delta = (1 - ss) / rho\n",
    "\n",
    "        # NOTE this is not exactly the same as in reference algo\n",
    "        # Fortunately it seems the clamped values never wrongly\n",
    "        # get selected by tau <= sorted_z. Prove this!\n",
    "        delta_nz = torch.clamp(delta, 0)\n",
    "        tau = mean - torch.sqrt(delta_nz)\n",
    "\n",
    "        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n",
    "        tau_star = tau.gather(dim, support_size - 1)\n",
    "        return tau_star, support_size\n",
    "\n",
    "\n",
    "class Entmoid15(Function):\n",
    "    \"\"\" A highly optimized equivalent of lambda x: Entmax15([x, 0]) \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = Entmoid15._forward(input)\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _forward(input):\n",
    "        input, is_pos = abs(input), input >= 0\n",
    "        tau = (input + torch.sqrt(F.relu(8 - input ** 2))) / 2\n",
    "        tau.masked_fill_(tau <= input, 2.0)\n",
    "        y_neg = 0.25 * F.relu(tau - input, inplace=True) ** 2\n",
    "        return torch.where(is_pos, 1 - y_neg, y_neg)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return Entmoid15._backward(ctx.saved_tensors[0], grad_output)\n",
    "\n",
    "    @staticmethod\n",
    "    def _backward(output, grad_output):\n",
    "        gppr0, gppr1 = output.sqrt(), (1 - output).sqrt()\n",
    "        grad_input = grad_output * gppr0\n",
    "        q = grad_input / (gppr0 + gppr1)\n",
    "        grad_input -= q * gppr0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "entmax15 = Entmax15Function.apply\n",
    "entmoid15 = Entmoid15.apply\n",
    "\n",
    "\n",
    "class Entmax15(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Entmax15, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return entmax15(input, self.dim)\n",
    "    \n",
    "    \n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    return\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, virtual_batch_size=512):\n",
    "        super(GBN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = nn.BatchNorm1d(self.input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training == True:\n",
    "            chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "            res = [self.bn(x_) for x_ in chunks]\n",
    "            return torch.cat(res, dim=0)\n",
    "        else:\n",
    "            return self.bn(x)\n",
    "\n",
    "class LearnableLocality(nn.Module):\n",
    "    def __init__(self, input_dim, k):\n",
    "        super(LearnableLocality, self).__init__()\n",
    "        self.register_parameter('weight', nn.Parameter(torch.rand(k, input_dim)))\n",
    "        self.smax = Entmax15(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.smax(self.weight)\n",
    "        masked_x = torch.einsum('nd,bd->bnd', mask, x)  # [B, k, D]\n",
    "        return masked_x\n",
    "\n",
    "class AbstractLayer(nn.Module):\n",
    "    def __init__(self, base_input_dim, base_output_dim, k, virtual_batch_size, bias=True):\n",
    "        super(AbstractLayer, self).__init__()\n",
    "        self.masker = LearnableLocality(input_dim=base_input_dim, k=k)\n",
    "        self.fc = nn.Conv1d(base_input_dim * k, 2 * k * base_output_dim, kernel_size=1, groups=k, bias=bias)\n",
    "        initialize_glu(self.fc, input_dim=base_input_dim * k, output_dim=2 * k * base_output_dim)\n",
    "        self.bn = GBN(2 * base_output_dim * k, virtual_batch_size)\n",
    "        self.k = k\n",
    "        self.base_output_dim = base_output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        x = self.masker(x)  # [B, D] -> [B, k, D]\n",
    "        x = self.fc(x.view(b, -1, 1))  # [B, k, D] -> [B, k * D, 1] -> [B, k * (2 * D'), 1]\n",
    "        x = self.bn(x)\n",
    "        chunks = x.chunk(self.k, 1)  # k * [B, 2 * D', 1]\n",
    "        x = sum([F.relu(torch.sigmoid(x_[:, :self.base_output_dim, :]) * x_[:, self.base_output_dim:, :]) for x_ in chunks])  # k * [B, D', 1] -> [B, D', 1]\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, input_dim, base_outdim, k, virtual_batch_size, fix_input_dim, drop_rate):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = AbstractLayer(input_dim, base_outdim // 2, k, virtual_batch_size)\n",
    "        self.conv2 = AbstractLayer(base_outdim // 2, base_outdim, k, virtual_batch_size)\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            AbstractLayer(fix_input_dim, base_outdim, k, virtual_batch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pre_out=None):\n",
    "        if pre_out == None:\n",
    "            pre_out = x\n",
    "        out = self.conv1(pre_out)\n",
    "        out = self.conv2(out)\n",
    "        identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.leaky_relu(out, 0.01)\n",
    "\n",
    "\n",
    "class DANet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_in,\n",
    "                 n_out,\n",
    "                 layer_num=20,\n",
    "                 base_outdim=64,\n",
    "                 k=5,\n",
    "                 virtual_batch_size=256,\n",
    "                 drop_rate=0.1,\n",
    "                 **kwargs,\n",
    "                ):\n",
    "        super(DANet, self).__init__()\n",
    "        params = {'base_outdim': base_outdim, 'k': k, 'virtual_batch_size': virtual_batch_size,\n",
    "                  'fix_input_dim': n_in, 'drop_rate': drop_rate}\n",
    "        self.init_layer = BasicBlock(n_in, **params)\n",
    "        self.lay_num = layer_num\n",
    "        self.layer = nn.ModuleList()\n",
    "        for i in range((layer_num // 2) - 1):\n",
    "            self.layer.append(BasicBlock(base_outdim, **params))\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(base_outdim, 256),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Linear(256, 512),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Linear(512, n_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_layer(x)\n",
    "        for i in range(len(self.layer)):\n",
    "            out = self.layer[i](x, out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,\n",
    "        n_out,\n",
    "        hidden_size=256,\n",
    "        drop_rate=0.2,\n",
    "        **kwargs, # kwargs is must-have to hold unnecessary parameters\n",
    "    ):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([]))\n",
    "        self.features.add_module(\"norm\", nn.BatchNorm1d(n_in))\n",
    "        self.features.add_module(\"dense1\", nn.Linear(n_in, hidden_size))\n",
    "        self.features.add_module(\"act\", nn.SiLU())\n",
    "        self.features.add_module(\"dropout\", nn.Dropout(p=drop_rate))\n",
    "        self.features.add_module(\"dense2\", nn.Linear(hidden_size, n_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.features:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:21:06] Stdout logging level is INFO.\n",
      "[20:21:06] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[20:21:06] Task: binary\n",
      "\n",
      "[20:21:06] Start automl preset with listed constraints:\n",
      "[20:21:06] - time: 54000000.00 seconds\n",
      "[20:21:06] - CPU: 24 cores\n",
      "[20:21:06] - memory: 16 GB\n",
      "\n",
      "[20:21:06] \u001b[1mTrain data shape: (257535, 3431)\u001b[0m\n",
      "\n",
      "[20:22:22] Layer \u001b[1m1\u001b[0m train process start. Time left 53999924.06 secs\n",
      "[20:22:55] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[20:35:13] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.8821279032716562\u001b[0m\n",
      "[20:35:13] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[20:35:13] Time left 53999152.44 secs\n",
      "\n",
      "[20:56:14] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[20:56:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[23:16:12] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.88855800722462\u001b[0m\n",
      "[23:16:12] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[23:16:13] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:19:45] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m finished. score = \u001b[1m0.8838861669990338\u001b[0m\n",
      "[23:19:45] \u001b[1mLvl_0_Pipe_1_Mod_1_CatBoost\u001b[0m fitting and predicting completed\n",
      "[23:19:45] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_2_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:25:11] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_2_Tuned_CatBoost\u001b[0m completed\n",
      "[23:25:11] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_Tuned_CatBoost\u001b[0m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:33:00] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8856357817061904\u001b[0m\n",
      "[23:33:00] \u001b[1mLvl_0_Pipe_1_Mod_2_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[23:33:00] Time left 53988485.37 secs\n",
      "\n",
      "[23:33:23] Start fitting \u001b[1mLvl_0_Pipe_2_Mod_0_TorchNN_0\u001b[0m ...\n",
      "[01:34:49] Fitting \u001b[1mLvl_0_Pipe_2_Mod_0_TorchNN_0\u001b[0m finished. score = \u001b[1m0.8863459801514103\u001b[0m\n",
      "[01:34:49] \u001b[1mLvl_0_Pipe_2_Mod_0_TorchNN_0\u001b[0m fitting and predicting completed\n",
      "[01:34:49] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1\u001b[0m ... Time budget is 3600.00 secs\n",
      "[02:37:53] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1\u001b[0m completed\n",
      "[02:37:53] Start fitting \u001b[1mLvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1\u001b[0m ...\n",
      "[03:16:50] Fitting \u001b[1mLvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1\u001b[0m finished. score = \u001b[1m0.8918195829070951\u001b[0m\n",
      "[03:16:50] \u001b[1mLvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1\u001b[0m fitting and predicting completed\n",
      "[03:16:50] Start fitting \u001b[1mLvl_0_Pipe_2_Mod_2_TorchNN_resnet_2\u001b[0m ...\n",
      "[03:44:53] Fitting \u001b[1mLvl_0_Pipe_2_Mod_2_TorchNN_resnet_2\u001b[0m finished. score = \u001b[1m0.8900824701534213\u001b[0m\n",
      "[03:44:53] \u001b[1mLvl_0_Pipe_2_Mod_2_TorchNN_resnet_2\u001b[0m fitting and predicting completed\n",
      "[03:44:53] Start fitting \u001b[1mLvl_0_Pipe_2_Mod_3_TorchNN_dense_3\u001b[0m ...\n",
      "[04:12:40] Fitting \u001b[1mLvl_0_Pipe_2_Mod_3_TorchNN_dense_3\u001b[0m finished. score = \u001b[1m0.8913935359214176\u001b[0m\n",
      "[04:12:40] \u001b[1mLvl_0_Pipe_2_Mod_3_TorchNN_dense_3\u001b[0m fitting and predicting completed\n",
      "[04:12:41] Start fitting \u001b[1mLvl_0_Pipe_2_Mod_4_TorchNN_denselight_4\u001b[0m ...\n",
      "[04:27:29] Fitting \u001b[1mLvl_0_Pipe_2_Mod_4_TorchNN_denselight_4\u001b[0m finished. score = \u001b[1m0.8913792959872164\u001b[0m\n",
      "[04:27:29] \u001b[1mLvl_0_Pipe_2_Mod_4_TorchNN_denselight_4\u001b[0m fitting and predicting completed\n",
      "[04:27:29] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5\u001b[0m ... Time budget is 3600.00 secs\n",
      "[05:18:15] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5\u001b[0m completed\n",
      "[05:18:16] Start fitting \u001b[1mLvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5\u001b[0m ...\n",
      "[05:37:57] Fitting \u001b[1mLvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5\u001b[0m finished. score = \u001b[1m0.8921900420570358\u001b[0m\n",
      "[05:37:57] \u001b[1mLvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5\u001b[0m fitting and predicting completed\n",
      "[05:37:57] Time left 53966588.99 secs\n",
      "\n",
      "[05:37:57] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[05:37:57] Blending: optimization starts with equal weights and score \u001b[1m0.8952668807261742\u001b[0m\n",
      "[05:38:00] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8962589101451709\u001b[0m, weights = \u001b[1m[0.02960312 0.211132   0.03161177 0.02945014 0.06940495 0.22730064\n",
      " 0.03121654 0.09875673 0.10045972 0.17106436]\u001b[0m\n",
      "[05:38:03] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8963175106176\u001b[0m, weights = \u001b[1m[0.02432299 0.23019485 0.02597337 0.0241973  0.07168575 0.22224776\n",
      " 0.02564864 0.11008801 0.09098504 0.17465626]\u001b[0m\n",
      "[05:38:05] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8963495415461157\u001b[0m, weights = \u001b[1m[0.02091912 0.24052554 0.02233853 0.02081101 0.07042781 0.22334681\n",
      " 0.02205924 0.11093609 0.09302241 0.17561348]\u001b[0m\n",
      "[05:38:08] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8963709937291788\u001b[0m, weights = \u001b[1m[0.01848459 0.24755867 0.01973882 0.01838906 0.07159262 0.2226179\n",
      " 0.01949204 0.11042109 0.0949581  0.17674713]\u001b[0m\n",
      "[05:38:11] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8963867024474734\u001b[0m, weights = \u001b[1m[0.01658329 0.25217667 0.01770851 0.01649759 0.07344954 0.22431073\n",
      " 0.0174871  0.10914534 0.09542458 0.17721668]\u001b[0m\n",
      "[05:38:11] \u001b[1mAutoml preset training completed in 33425.66 seconds\u001b[0m\n",
      "\n",
      "[05:38:11] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.01658 * (10 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.25218 * (10 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.01771 * (10 averaged models Lvl_0_Pipe_1_Mod_1_CatBoost) +\n",
      "\t 0.01650 * (10 averaged models Lvl_0_Pipe_1_Mod_2_Tuned_CatBoost) +\n",
      "\t 0.07345 * (10 averaged models Lvl_0_Pipe_2_Mod_0_TorchNN_0) +\n",
      "\t 0.22431 * (10 averaged models Lvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1) +\n",
      "\t 0.01749 * (10 averaged models Lvl_0_Pipe_2_Mod_2_TorchNN_resnet_2) +\n",
      "\t 0.10915 * (10 averaged models Lvl_0_Pipe_2_Mod_3_TorchNN_dense_3) +\n",
      "\t 0.09542 * (10 averaged models Lvl_0_Pipe_2_Mod_4_TorchNN_denselight_4) +\n",
      "\t 0.17722 * (10 averaged models Lvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.automl.presets.text_presets import TabularNLPAutoML\n",
    "from lightautoml.tasks import Task\n",
    "import time\n",
    "\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "roles = {'target': 'is_male',\n",
    "                 'drop': ['user_id', 'index', \"age\"]}\n",
    "default_nn_params = {\n",
    "    \"n_epochs\": 200,\n",
    "}\n",
    "default_nn_params_2 = {\n",
    "    \"n_epochs\": 100,\n",
    "}\n",
    "automl = TabularAutoML(\n",
    "    task = Task(name='binary'),\n",
    "    cpu_limit = 24,\n",
    "    timeout=3600*15000,\n",
    "    gpu_ids='0',\n",
    "    debug=True,\n",
    "    tuning_params = {'max_tuning_iter': 100},\n",
    "    general_params = {\"use_algos\": [[DANet, \"resnet_tuned\", \"resnet\", \"dense\", \"denselight\",\n",
    "                                     \"linear_l2\", \"lgb\", \"cb\", \"cb_tuned\", \"denselight_tuned\"]], \n",
    "                      'return_all_predictions': True,\n",
    "                      'weighted_blender_max_nonzero_coef': 0.0},\n",
    "    nn_pipeline_params = {\"use_te\": True,  \"max_intersection_depth\": 0},\n",
    "    nn_params = {\"0\":{**default_nn_params},\n",
    "                 \"1\":{**default_nn_params_2}},\n",
    "    reader_params = {'cv': 10, 'random_state': 42}\n",
    ")\n",
    "start_time = time.time()\n",
    "oof_pred = automl.fit_predict(\n",
    "        train,\n",
    "        roles=roles,\n",
    "        verbose = 1\n",
    "    )\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spend time: 33425.671926021576\n",
      "age ca roc auc: 0.8821279032716562\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.01658 * (10 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.25218 * (10 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.01771 * (10 averaged models Lvl_0_Pipe_1_Mod_1_CatBoost) +\n",
      "\t 0.01650 * (10 averaged models Lvl_0_Pipe_1_Mod_2_Tuned_CatBoost) +\n",
      "\t 0.07345 * (10 averaged models Lvl_0_Pipe_2_Mod_0_TorchNN_0) +\n",
      "\t 0.22431 * (10 averaged models Lvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1) +\n",
      "\t 0.01749 * (10 averaged models Lvl_0_Pipe_2_Mod_2_TorchNN_resnet_2) +\n",
      "\t 0.10915 * (10 averaged models Lvl_0_Pipe_2_Mod_3_TorchNN_dense_3) +\n",
      "\t 0.09542 * (10 averaged models Lvl_0_Pipe_2_Mod_4_TorchNN_denselight_4) +\n",
      "\t 0.17722 * (10 averaged models Lvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5) \n"
     ]
    }
   ],
   "source": [
    "print(f\"spend time: {end_time - start_time}\")\n",
    "pred_ismale = oof_pred.data[: ,0]\n",
    "pred_ismale[np.isnan(pred_ismale)] = 0.5\n",
    "print(f\"age ca roc auc: {roc_auc_score(train['is_male'], pred_ismale)}\")\n",
    "print(automl.create_model_str_desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72084</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14368</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130234</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100995</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14369</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id\n",
       "72084         6\n",
       "14368         7\n",
       "130234        9\n",
       "100995       10\n",
       "14369        11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_ismale = automl.predict(test)\n",
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = test.user_id\n",
    "sub[list(automl.collect_model_stats().keys())] = test_pred_ismale.data\n",
    "sub.sort_values(\"user_id\", inplace=True)\n",
    "sub.to_csv(\"test_warm_male_big_automl.csv\", index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>Lvl_0_Pipe_0_Mod_0_LinearL2</th>\n",
       "      <th>Lvl_0_Pipe_1_Mod_0_LightGBM</th>\n",
       "      <th>Lvl_0_Pipe_1_Mod_1_CatBoost</th>\n",
       "      <th>Lvl_0_Pipe_1_Mod_2_Tuned_CatBoost</th>\n",
       "      <th>Lvl_0_Pipe_2_Mod_0_TorchNN_0</th>\n",
       "      <th>Lvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1</th>\n",
       "      <th>Lvl_0_Pipe_2_Mod_2_TorchNN_resnet_2</th>\n",
       "      <th>Lvl_0_Pipe_2_Mod_3_TorchNN_dense_3</th>\n",
       "      <th>Lvl_0_Pipe_2_Mod_4_TorchNN_denselight_4</th>\n",
       "      <th>Lvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.056811</td>\n",
       "      <td>0.048226</td>\n",
       "      <td>0.031019</td>\n",
       "      <td>0.032150</td>\n",
       "      <td>0.066619</td>\n",
       "      <td>0.031741</td>\n",
       "      <td>0.040632</td>\n",
       "      <td>0.025944</td>\n",
       "      <td>0.021272</td>\n",
       "      <td>0.037604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.858543</td>\n",
       "      <td>0.837406</td>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.626704</td>\n",
       "      <td>0.701507</td>\n",
       "      <td>0.666221</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>0.720842</td>\n",
       "      <td>0.708428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.995208</td>\n",
       "      <td>0.985653</td>\n",
       "      <td>0.984499</td>\n",
       "      <td>0.991210</td>\n",
       "      <td>0.992509</td>\n",
       "      <td>0.993675</td>\n",
       "      <td>0.991972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>0.124199</td>\n",
       "      <td>0.211962</td>\n",
       "      <td>0.140921</td>\n",
       "      <td>0.119063</td>\n",
       "      <td>0.038975</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>0.070326</td>\n",
       "      <td>0.102647</td>\n",
       "      <td>0.111554</td>\n",
       "      <td>0.089488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>0.245797</td>\n",
       "      <td>0.418034</td>\n",
       "      <td>0.354164</td>\n",
       "      <td>0.424152</td>\n",
       "      <td>0.043008</td>\n",
       "      <td>0.126105</td>\n",
       "      <td>0.142034</td>\n",
       "      <td>0.071950</td>\n",
       "      <td>0.081098</td>\n",
       "      <td>0.180981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  Lvl_0_Pipe_0_Mod_0_LinearL2  Lvl_0_Pipe_1_Mod_0_LightGBM  \\\n",
       "0        4                     0.056811                     0.048226   \n",
       "1       16                     0.584372                     0.858543   \n",
       "2       18                     0.997506                     0.994687   \n",
       "3       26                     0.124199                     0.211962   \n",
       "4       29                     0.245797                     0.418034   \n",
       "\n",
       "   Lvl_0_Pipe_1_Mod_1_CatBoost  Lvl_0_Pipe_1_Mod_2_Tuned_CatBoost  \\\n",
       "0                     0.031019                           0.032150   \n",
       "1                     0.837406                           0.854719   \n",
       "2                     0.995327                           0.995208   \n",
       "3                     0.140921                           0.119063   \n",
       "4                     0.354164                           0.424152   \n",
       "\n",
       "   Lvl_0_Pipe_2_Mod_0_TorchNN_0  \\\n",
       "0                      0.066619   \n",
       "1                      0.626704   \n",
       "2                      0.985653   \n",
       "3                      0.038975   \n",
       "4                      0.043008   \n",
       "\n",
       "   Lvl_0_Pipe_2_Mod_1_Tuned_TorchNN_resnet_tuned_1  \\\n",
       "0                                         0.031741   \n",
       "1                                         0.701507   \n",
       "2                                         0.984499   \n",
       "3                                         0.116071   \n",
       "4                                         0.126105   \n",
       "\n",
       "   Lvl_0_Pipe_2_Mod_2_TorchNN_resnet_2  Lvl_0_Pipe_2_Mod_3_TorchNN_dense_3  \\\n",
       "0                             0.040632                            0.025944   \n",
       "1                             0.666221                            0.674068   \n",
       "2                             0.991210                            0.992509   \n",
       "3                             0.070326                            0.102647   \n",
       "4                             0.142034                            0.071950   \n",
       "\n",
       "   Lvl_0_Pipe_2_Mod_4_TorchNN_denselight_4  \\\n",
       "0                                 0.021272   \n",
       "1                                 0.720842   \n",
       "2                                 0.993675   \n",
       "3                                 0.111554   \n",
       "4                                 0.081098   \n",
       "\n",
       "   Lvl_0_Pipe_2_Mod_5_Tuned_TorchNN_denselight_tuned_5  \n",
       "0                                           0.037604    \n",
       "1                                           0.708428    \n",
       "2                                           0.991972    \n",
       "3                                           0.089488    \n",
       "4                                           0.180981    "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = pd.DataFrame()\n",
    "oof['user_id'] = train.user_id\n",
    "oof[list(automl.collect_model_stats().keys())] = oof_pred.data\n",
    "oof.head()\n",
    "oof.to_csv(\"oof_warm_male_big_automl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
